{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datasets import load_dataset\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, normalized_mutual_info_score, adjusted_rand_score\n",
        "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score  # Fixed import\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split  # Added for validation split\n",
        "\n",
        "# Set seeds and device\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = load_dataset(\"mnist\")\n",
        "\n",
        "# Preprocess\n",
        "train_images = np.array([np.array(img).reshape(28, 28) for img in data[\"train\"][\"image\"]]) / 255.0\n",
        "test_images = np.array([np.array(img).reshape(28, 28) for img in data[\"test\"][\"image\"]]) / 255.0\n",
        "train_labels = np.array(data[\"train\"][\"label\"])\n",
        "test_labels = np.array(data[\"test\"][\"label\"])\n",
        "\n",
        "# Add channel dimension\n",
        "train_tensor = torch.tensor(train_images[:, None, :, :], dtype=torch.float32)\n",
        "test_tensor = torch.tensor(test_images[:, None, :, :], dtype=torch.float32)\n",
        "\n",
        "# Split training data into train and validation\n",
        "train_idx, val_idx = train_test_split(range(len(train_tensor)), test_size=0.2, random_state=42)\n",
        "train_subset = train_tensor[train_idx]\n",
        "val_tensor = train_tensor[val_idx]\n",
        "\n",
        "# Dataset\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(TensorDataset(train_subset), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(val_tensor), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model\n",
        "class TripletAutoencoderCNN(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, stride=2, padding=1),  # 14x14\n",
        "            nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 7x7\n",
        "            nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7, latent_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64*7*7), nn.ReLU(),\n",
        "            nn.Unflatten(1, (64, 7, 7)),\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        z = F.normalize(z, p=2, dim=1)  # Normalize latent vectors\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat, z\n",
        "\n",
        "model = TripletAutoencoderCNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "recon_loss = nn.MSELoss()\n",
        "\n",
        "# Triplet mining (unsupervised)\n",
        "def mine_triplets(data_tensor, model, n_samples=10000):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _, embeddings = model(data_tensor.to(device))\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "    triplets = []\n",
        "    for i in range(n_samples):\n",
        "        anchor = embeddings[i]\n",
        "        dists = np.linalg.norm(embeddings - anchor, axis=1)\n",
        "        pos_idx = np.argmin(np.where(dists == 0, np.inf, dists))\n",
        "        neg_pool = np.where(dists > 0.5)[0]\n",
        "        if len(neg_pool) == 0:\n",
        "            continue\n",
        "        neg_idx = neg_pool[np.argmax(dists[neg_pool])]\n",
        "        triplets.append((data_tensor[i], data_tensor[pos_idx], data_tensor[neg_idx]))\n",
        "    return triplets\n",
        "\n",
        "# Triplet Loss\n",
        "def triplet_loss(a, p, n, margin=1.0):\n",
        "    return F.relu(torch.norm(a - p, dim=1) - torch.norm(a - n, dim=1) + margin).mean()\n",
        "\n",
        "# Phase 1: Train autoencoder\n",
        "for epoch in range(12):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for (x,) in train_loader:\n",
        "        x = x.to(device)\n",
        "        x_hat, z = model(x)\n",
        "        loss = recon_loss(x_hat, x)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for (x,) in val_loader:\n",
        "            x = x.to(device)\n",
        "            x_hat, _ = model(x)\n",
        "            total_val_loss += recon_loss(x_hat, x).item()\n",
        "    print(f\"Epoch {epoch+1}, Train Recon Loss: {total_train_loss/len(train_loader):.4f}, Val Recon Loss: {total_val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "# Phase 2: Triplet training\n",
        "triplets = mine_triplets(train_subset[:20000], model)  # Use train_subset\n",
        "val_triplets = mine_triplets(val_tensor[:5000], model, n_samples=2000)  # Validation triplets\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for (a, p, n) in triplets:\n",
        "        a, p, n = a.unsqueeze(0).to(device), p.unsqueeze(0).to(device), n.unsqueeze(0).to(device)\n",
        "        _, a_z = model(a)\n",
        "        _, p_z = model(p)\n",
        "        _, n_z = model(n)\n",
        "        loss = triplet_loss(a_z, p_z, n_z)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for (a, p, n) in val_triplets:\n",
        "            a, p, n = a.unsqueeze(0).to(device), p.unsqueeze(0).to(device), n.unsqueeze(0).to(device)\n",
        "            _, a_z = model(a)\n",
        "            _, p_z = model(p)\n",
        "            _, n_z = model(n)\n",
        "            total_val_loss += triplet_loss(a_z, p_z, n_z).item()\n",
        "    print(f\"Triplet Epoch {epoch+1}, Train Triplet Loss: {total_train_loss/len(triplets):.4f}, Val Triplet Loss: {total_val_loss/len(val_triplets):.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    _, z_test = model(test_tensor.to(device))\n",
        "    z_test = z_test.cpu().numpy()\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42).fit(z_test)\n",
        "preds = kmeans.labels_\n",
        "\n",
        "# In evaluation section, after KMeans\n",
        "print(\"\\nEvaluation on MNIST:\")\n",
        "print(f\"Silhouette Score: {silhouette_score(z_test, preds):.4f}\")\n",
        "print(f\"Davies-Bouldin Index: {davies_bouldin_score(z_test, preds):.4f}\")\n",
        "print(f\"Calinski-Harabasz Index: {calinski_harabasz_score(z_test, preds):.4f}\")\n",
        "print(f\"NMI: {normalized_mutual_info_score(test_labels, preds):.4f}\")\n",
        "print(f\"ARI: {adjusted_rand_score(test_labels, preds):.4f}\")\n",
        "\n",
        "# t-SNE\n",
        "# t-SNE visualization (full test set)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "z_2d = tsne.fit_transform(z_test)\n",
        "sns.scatterplot(x=z_2d[:,0], y=z_2d[:,1], hue=preds, palette=\"tab10\", s=10)\n",
        "plt.title(\"t-SNE of Clustered Embeddings (Full Test Set)\")\n",
        "plt.show()\n",
        "# -------------------------------\n",
        "# Parameter Count\n",
        "# -------------------------------\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"\\nTotal Trainable Parameters: {total_params}\")\n"
      ],
      "metadata": {
        "id": "U6qzVyJhFBb-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}